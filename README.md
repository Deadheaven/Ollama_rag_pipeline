# Ollam_rag_pipeline
This project implements a completely local RAG (Retrieval-Augmented Generation) system powered by LangChain, FAISS, and Ollama. It loads PDF documents, embeds them using nomic-embed-text, stores vectors in FAISS (no SQLite or cloud services), and queries them with local LLMs like Qwen for contextual Q&amp;A and summarization.
