# ğŸ§© Local RAG Pipeline with Ollama + FAISS

A lightweight **Retrieval-Augmented Generation (RAG)** pipeline that runs **entirely locally** â€” no cloud APIs required.  
Built using **LangChain**, **FAISS**, and **Ollama** with open-source models like Qwen and Nomic.

---

## âœ¨ Features

- ğŸ“„ Load and index local PDF documents
- ğŸ§  Embed using Ollamaâ€™s `nomic-embed-text`
- ğŸ” Retrieve context chunks via FAISS (no SQLite)
- ğŸ¤– Query locally with Qwen, Mistral, or LLaMA models
- ğŸ’¾ 100% local â€” no API keys, no internet dependency

---

## ğŸ§± Tech Stack

| Component | Purpose |
|------------|----------|
| [LangChain](https://python.langchain.com/) | RAG and retrieval orchestration |
| [FAISS](https://github.com/facebookresearch/faiss) | Vector similarity search |
| [Ollama](https://ollama.ai) | Local LLM & embedding inference |
| [PyPDF](https://pypi.org/project/pypdf/) | PDF text extraction |

---

## âš™ï¸ Installation

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/rag-ollama-faiss.git
cd rag-ollama-faiss

# 2ï¸âƒ£ Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Start Ollama service (in another terminal)
ollama serve

# 5ï¸âƒ£ Run the script
python rag_local_faiss.py
